---
title: "Final Group project"
author: "Group 11: Jacqui Cole, Brent Lewis, Saagar Hemrajani, Vaani Kohli"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---


```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(skimr)
library(kknn)
library(here)
library(tictoc)
library(vip)
library(ranger)
```

# The problem: predicting credit card fraud

The goal of the project is to predict fraudulent credit card transactions.

We will be using a dataset with credit card transactions containing legitimate and fraud transactions. Fraud is typically well below 1% of all transactions, so a naive model that predicts that all transactions are legitimate and not fraudulent would have an accuracy of well over 99%-- pretty good, no? 

You can read more on credit card fraud on [Credit Card Fraud Detection Using Weighted Support Vector Machine](https://www.scirp.org/journal/paperinformation.aspx?paperid=105944)

The dataset we will use consists of credit card transactions and it includes information about each transaction including customer details, the merchant and category of purchase, and whether or not the transaction was a fraud.

## Obtain the data

The dataset is too large to be hosted on Canvas or Github, so please download it from dropbox https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0 and save it in your `dsb` repo, under the `data` folder.

As we will be building a classifier model using tidymodels, there's two things we need to do:

1. Define the outcome variable `is_fraud` as a factor, or categorical, variable, instead of the numerical 0-1 varaibles.
2. In tidymodels, the first level is the event of interest. If we leave our data as is, `0` is the first level, but we want to find out when we actually did (`1`) have a fraudulent transaction

```{r}
#| echo: false
#| message: false
#| warning: false

card_fraud <- read_csv(here::here("data", "card_fraud.csv")) %>% 

  mutate(
    # in tidymodels, outcome should be a factor  
    is_fraud = factor(is_fraud),
    
    # first level is the event in tidymodels, so we need to reorder
    is_fraud = relevel(is_fraud, ref = "1")
         )

glimpse(card_fraud)
```

The data dictionary is as follows

| column(variable)      | description                                 |
|-----------------------|---------------------------------------------|
| trans_date_trans_time | Transaction DateTime                        |
| trans_year            | Transaction year                            |
| category              | category of merchant                        |
| amt                   | amount of transaction                       |
| city                  | City of card holder                         |
| state                 | State of card holder                        |
| lat                   | Latitude location of purchase               |
| long                  | Longitude location of purchase              |
| city_pop              | card holder's city population               |
| job                   | job of card holder                          |
| dob                   | date of birth of card holder                |
| merch_lat             | Latitude Location of Merchant               |
| merch_long            | Longitude Location of Merchant              |
| is_fraud              | Whether Transaction is Fraud (1) or Not (0) |

We also add some of the variables we considered in our EDA for this dataset during homework 2.

```{r}
card_fraud <- card_fraud %>% 
  mutate( hour = hour(trans_date_trans_time),
          wday = wday(trans_date_trans_time, label = TRUE),
          month_name = month(trans_date_trans_time, label = TRUE),
          age = interval(dob, trans_date_trans_time) / years(1)
) %>% 
  rename(year = trans_year) %>% 
  
  mutate(
    
    # convert latitude/longitude to radians
    lat1_radians = lat / 57.29577951,
    lat2_radians = merch_lat / 57.29577951,
    long1_radians = long / 57.29577951,
    long2_radians = merch_long / 57.29577951,
    
    # calculate distance in miles
    distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)),

    # calculate distance in km
    distance_km = 6377.830272 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians))

  )

```

## Exploratory Data Analysis (EDA) 

You have done some EDA and you can pool together your group's expertise in which variables to use as features.
You can reuse your EDA from earlier, but we expect at least a few visualisations and/or tables to explore teh dataset and identify any useful features.

Group all variables by type and examine each variable class by class. The dataset has the following types of variables:

```{r}

card_fraud %>% 
  group_by(category) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>% 
  mutate(pct = count/nrow(card_fraud)) %>% 
  mutate(category = fct_rev(fct_reorder(category, pct, max))) %>%
  ggplot(aes(x =category, y=pct))+
  geom_bar(position = 'dodge', stat='identity')+
  theme_minimal(base_size=6) +
  ggtitle("Most Transactions are Gas and Grocery") +
  scale_y_continuous(labels = scales::percent) +
  ggthemes::theme_economist() +
  theme(plot.title = element_text(size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 0.85))  # Adjust the size value as needed


num_fraud <- card_fraud %>%
  filter(is_fraud == 1) %>%
  count() %>% 
  pull(n)

num_category <- card_fraud %>%
  group_by(category) %>%
  summarise(category_count = n())


rate_by_cat <- card_fraud %>%
  filter(is_fraud == 1 & !is.na(category)) %>% 
  group_by(category) %>% 
  summarise(category_fraud_count = n()) %>% 
  left_join(num_category, by='category') %>% 
  mutate(cat_fraud_pct = category_fraud_count/category_count) %>% 
  arrange(desc(cat_fraud_pct)) 
  
  
rate_by_cat %>%   
  mutate(category = fct_rev(fct_reorder(category, cat_fraud_pct, max))) %>%
  ggplot(aes(x =category, y=cat_fraud_pct))+
  geom_bar(position = 'dodge', stat='identity')+
  theme_minimal(base_size=6) +
  ggtitle("Grocery and Online Shopping have highest percentage of fraud") +
  scale_y_continuous(labels = scales::percent) +
  ggthemes::theme_economist() +
  theme(plot.title = element_text(size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 0.85))  # Adjust the size value as needed


card_fraud_date <- card_fraud %>% 
                  mutate(
                    date_only = lubridate::date(trans_date_trans_time),
                    month_name = lubridate::month(trans_date_trans_time, label=TRUE),
                    month_number = month(date(trans_date_trans_time)),
                    hour = lubridate::hour(trans_date_trans_time),
                    weekday = lubridate::wday(trans_date_trans_time, label = TRUE),
                    weekday_num = lubridate::wday(trans_date_trans_time, label = FALSE)
                    )

card_fraud_date %>%
  filter(is_fraud == 1 & !is.na(month_name)) %>%
  mutate(month_name = fct_reorder(month_name, month_number, max)) %>%
  group_by(month_name, month_number) %>%
  summarize(pct_fraud_transactions = n() / num_fraud * 100) %>%  
  ggplot(aes(x = month_name, y = pct_fraud_transactions)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Fraud is more likely to occur in the first half of the year than the second",
       x = "",
       y = "Percentage of Fradulent Transactions",
       subtitle = "With march and may having\nhighest percentage of fraudulent transactions") +
  ggthemes::theme_economist() + 
  theme(plot.title = element_text(size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 0.85))


card_fraud_date %>%
  filter(is_fraud == 1 & !is.na(hour)) %>%
  arrange(hour) %>% 
  group_by(hour) %>%
  summarize(pct_fraud_transactions = n() / num_fraud * 100) %>%  
  ggplot(aes(x = hour, y = pct_fraud_transactions)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_x_continuous(breaks = seq(0, 23, by = 1)) +
  labs(title = "A lot of the fraudulent transactions occur just before midnight\nor in the late hours of the night",
       x = "Hour",
       y = "Percentage of Fradulent Transactions",
       subtitle = "11pm to midnight is the most common time for fraudulent transactions to occur") +
  ggthemes::theme_economist() + 
  theme(plot.title = element_text(size = 14),
        axis.text.x = element_text(hjust = 0.5, vjust = 0.85))


card_fraud_date %>%
  filter(is_fraud == 1 & !is.na(weekday)) %>%
  mutate(weekday = fct_reorder(weekday, weekday_num, max)) %>%
  group_by(weekday, weekday_num) %>%
  summarize(pct_fraud_transactions = n() / num_fraud * 100) %>%  
  ggplot(aes(x = weekday, y = pct_fraud_transactions)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "The weekends and monday are the most likely days for fraud",
       x = "",
       y = "Percentage of Fradulent Transactions",
       subtitle = "Wednesday is the least likely day") +
  ggthemes::theme_economist() + 
  theme(plot.title = element_text(size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 0.85))
```


1.  Strings
2.  Geospatial Data
3.  Dates
4.  Date/Times
5.  Numerical

Strings are usually not a useful format for classification problems. The strings should be converted to factors, dropped, or otherwise transformed.


***Strings to Factors*** 

-   `category`, Category of Merchant
-   `job`, Job of Credit Card Holder

***Strings to Geospatial Data*** 

We have plenty of geospatial data as lat/long pairs, so I want to convert city/state to lat/long so I can compare to the other geospatial variables. This will also make it easier to compute new variables like the distance the transaction is from the home location. 

-   `city`, City of Credit Card Holder
-   `state`, State of Credit Card Holder

##  Exploring factors: how is the compactness of categories?

-   Do we have excessive number of categories? Do we want to combine some?

```{r}
card_fraud %>% 
  count(category, sort=TRUE)%>% 
  mutate(perc = n/sum(n))

card_fraud %>% 
  count(job, sort=TRUE) %>% 
  mutate(perc = n/sum(n))


```


The predictors `category` and `job` are transformed into factors.

```{r}
#| label: convert-strings-to-factors


card_fraud <- card_fraud %>% 
  mutate(category = factor(category),
         job = factor(job),
         hour = factor(hour))

```

`category` has 14 unique values, and `job` has 494 unique values. The dataset is quite large, with over 670K records, so these variables don't have an excessive number of levels at first glance. However, it is worth seeing if we can compact the levels to a smaller number.

### Why do we care about the number of categories and whether they are "excessive"?

Consider the extreme case where a dataset had categories that only contained one record each. There is simply insufficient data to make correct predictions using category as a predictor on new data with that category label. Additionally, if your modeling uses dummy variables, having an extremely large number of categories will lead to the production of a huge number of predictors, which can slow down the fitting. This is fine if all the predictors are useful, but if they aren't useful (as in the case of having only one record for a category), trimming them will improve the speed and quality of the data fitting.

If I had subject matter expertise, I could manually combine categories. If you don't have subject matter expertise, or if performing this task would be too labor intensive, then you can use cutoffs based on the amount of data in a category. If the majority of the data exists in only a few categories, then it might be reasonable to keep those categories and lump everything else in an "other" category or perhaps even drop the data points in smaller categories. 


## Do all variables have sensible types?

Consider each variable and decide whether to keep, transform, or drop it. This is a mixture of Exploratory Data Analysis and Feature Engineering, but it's helpful to do some simple feature engineering as you explore the data. In this project, we have all data to begin with, so any transformations will be performed on the entire dataset. Ideally, do the transformations as a `recipe_step()` in the tidymodels framework. Then the transformations would be applied to any data the recipe was used on as part of the modeling workflow. There is less chance of data leakage or missing a step when you perform the feature engineering in the recipe.

## Which variables to keep in your model?

You have a number of variables and you have to decide which ones to use in your model. For instance, you have the latitude/lognitude of the customer, that of the merchant, the same data in radians, as well as the `distance_km` and `distance_miles`. Do you need them all? 

```{r}
card_fraud %>%
  glimpse
```


## Fit your workflows in smaller sample

You will be running a series of different models, along the lines of the California housing example we have seen in class. However, this dataset has 670K rows and if you try various models and run cross validation on them, your computer may slow down or crash.

Thus, we will work with a smaller sample of 10% of the values the original dataset to identify the best model, and once we have the best model we can use the full dataset to train- test our best model.

```{r}
# RATIONALE BEHIND OUR DECISION-MAKING PROCESS TO DETERMINE WHICH FEATURES TO CHOOSE FOR THE CREATION OF OUR CLASSIFICATION-BASED MACHINE-LEARNING MODEL THAT PREDICTS IF A TRANSACTION IS FRAUDULENT OR NOT.
#
# Considering each categories
#
# Strings
#
# category - we saw from our homework2 that fraud is linked to the category so we need to include this as a feature
#
# So, we short list: category
#
# job - we considered this but can see that there are far too many jobs listed that this data category is not useful as it is too fine
# grained for our purposes. Hence, we drop this variable.
# 
# city, states and city_pop are likely correlated to each other. We are also not sure that these variables will impact significantly upon the likelihood of fraud. We decide that it is possible but we do not include any of these variables as a feature for the model for the time being, since we wish to see how the training and testing of our model fares with the other features that we have already chosen. Indeed, we don't wish to employ too many features because that could overfit the model. We could come back to considering the addition of one of these city/country variables later if need be, once we see if our model works or not with our other features.
#
# Geospatial data
#
# A range of these type of data are given which we realise are highly correlated, i.e. latitude and longitude of the transaction (lat, long), the latitude and longitude of the merchant (merch_lat, merch_long), the radius of one and the other latitude and longitude (lat1_radius and lat2_radius, and long1_radius, and long2_radius), distance between the merchant and card holder that is given in both km and miles (distance_miles and distance_km).
#
# It only feels sensible to choose distance_km since its calculation is based on the latitude and longitude of both merchant and card holder, and the radius of these variables. Meanwhile, we have to choose between a distance as a km or miles unit, so we simply decide on the metric option: distance_km.
#
# So, we short list: distance_km
#
# Dates and Times
#
# The dob variable is dropped because we intuit that the day or month of someone's birth will not impact on whether they are a target of fraud or not. However, their year of birth could impact on the likelihood of being a target of fraud or otherwise. One of the other variables, age, already exists as a proxy to the year of birth and so we decide that we can simply drop the dob, and use age for our purposes.
#
# So we short list: age
#
# We intuit that trans_date_trans_time will affect the likelihood of a fraudulent transaction (see homework 2). The trans_date_trans_time will need to be split up into hour, day, month and year, and whether or not the date was a weekday or not, but this has already been done for us, given the following variables are given: hour, wday, and month_name. So trans_date_trans_time is considered to be redundant. Homework2 also showed that the evidence was unclear for any correlation between year and fraud likelihood.
#
# So, we short list: hour, wday, month_name
#
# Numericals
#
# We intuit that the amount (amt) of money in the card transaction is surely a feature because it is more attractive to commit fraud when there is a great 'return on (fraud) investment'. Indeed, we saw in Homework2 that the amt variable is significantly correlated to the likelihood of fraudulent behaviour or otherwise.
#
# In summary, the above text has considered all variables in the card_fraud dataset in terms of whether or not they should be selected as any of our features for creating our machine-learning (classification) model which predicts whether nor not fraud occurs or not (cf the is_fraud variable). We concluded that the following variables should go forward as features:
#
# category, amt, hour, wday, month_name, age, distance_km
#

```



```{r}
# select a smaller subset
my_card_fraud <- card_fraud %>% 
  select(is_fraud, category, amt, hour, wday, month_name, age, distance_km) %>% 
  slice_sample(prop = 0.01) %>% 
  mutate(hour = as.factor(hour))
```


## Split the data in training - testing

```{r}
# **Split the data**

set.seed(123)

data_split <- initial_split(my_card_fraud, # updated data
                           prop = 0.8, 
                           strata = is_fraud)

card_fraud_train <- training(data_split) 
card_fraud_test <- testing(data_split)
```


## Cross Validation

Start with 3 CV folds to quickly get an estimate for the best model and you can increase the number of folds to 5 or 10 later.

```{r}
set.seed(123)
cv_folds <- vfold_cv(data = card_fraud_train, 
                          v = 3, 
                          strata = is_fraud)
cv_folds 
```


## Define a tidymodels `recipe`

What steps are you going to add to your recipe? Do you need to do any log transformations?

```{r, define_recipe}

fraud_rec <- recipe(is_fraud ~ ., 
                    data = card_fraud_train) %>%
              step_log(amt, distance_km) %>% 
              step_naomit(everything(), skip = TRUE) %>% 
              step_novel(all_nominal(), -all_outcomes()) %>%
              step_normalize(all_numeric(), -all_outcomes()) %>%
              step_dummy(all_nominal(), -all_outcomes()) %>%
              step_zv(all_numeric(), -all_outcomes())

# - `step_novel()` converts all nominal variables to factors and takes care of other issues related to categorical variables.
# 
# - `step_log()` will log transform data (since some of our numerical variables are right-skewed). Note that this step can not be performed on negative numbers.
# 
# - `step_normalize()` normalizes (center and scales) the numeric variables to have a standard deviation of one and a mean of zero. (i.e., z-standardization).
# 
# - `step_dummy()` converts our factor column ocean_proximity into numeric binary (0 and 1) variables.
# 
# `step_zv()` removes any numeric variables that have zero variance.
# 
# `step_corr()` will remove predictor variables with high correlations with other predictor variables.


```

Once you have your recipe, you can check the pre-processed dataframe 

```{r}
prepped_data <- 
  fraud_rec %>% # use the recipe object
  prep() %>% # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(prepped_data)

```


## Define various models

You should define the following classification models:

1. Logistic regression, using the `glm` engine
2. Decision tree, using the `C5.0` engine
3. Random Forest, using  the `ranger` engine and setting `importance = "impurity"`)  
4. A boosted tree using Extreme Gradient Boosting, and the `xgboost` engine
5. A k-nearest neighbours,  using 4 nearest_neighbors and the `kknn` engine  

```{r, define_models}
## Model Building 

# 1. Pick a `model type`
# 2. set the `engine`
# 3. Set the `mode`:  classification

# Logistic regression
log_spec <-  logistic_reg() %>%  # model type
  set_engine(engine = "glm") %>%  # model engine
  set_mode("classification") # model mode

# Show your model specification
log_spec

# Decision Tree
tree_spec <- decision_tree() %>%
  set_engine(engine = "C5.0") %>%
  set_mode("classification")

tree_spec

# Random Forest
library(ranger)

rf_spec <- 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")


# Boosted tree (XGBoost)
library(xgboost)

xgb_spec <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# K-nearest neighbour (k-NN)
knn_spec <- 
  nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors 
  set_engine("kknn") %>% 
  set_mode("classification") 

```

## Bundle recipe and model with `workflows`

```{r, define_workflows}


## Bundle recipe and model with `workflows`


log_wflow <- # new workflow object
 workflow() %>% # use workflow function
 add_recipe(fraud_rec) %>%   # use the new recipe
 add_model(log_spec)   # add your model spec

tree_wflow <-
 workflow() %>%
 add_recipe(fraud_rec) %>% 
 add_model(tree_spec) 

rf_wflow <-
 workflow() %>%
 add_recipe(fraud_rec) %>% 
 add_model(rf_spec) 

xgb_wflow <-
 workflow() %>%
 add_recipe(fraud_rec) %>% 
 add_model(xgb_spec)

knn_wflow <-
 workflow() %>%
 add_recipe(fraud_rec) %>% 
 add_model(knn_spec)



# show object
knn_wflow


```


## Fit models

You may want to compare the time it takes to fit each model. `tic()` starts a simple timer and `toc()` stops it

```{r, fit_models}



tic()
log_res <- log_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, accuracy,
      kap, roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)) 
time <- toc()
fit_time_models <- c(time[[4]])


tic()
tree_res <- tree_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, accuracy,
      kap, roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)) 
time <- toc()
fit_time_models <- append(fit_time_models,time[[4]])

tic()
rf_res <- rf_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, accuracy,
      kap, roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)) 
time <- toc()
fit_time_models <- append(fit_time_models,time[[4]])

tic()
xgb_res <- xgb_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, accuracy,
      kap, roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)) 
time <- toc()
fit_time_models <- append(fit_time_models,time[[4]])

tic()
knn_res <- knn_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, accuracy,
      kap, roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)) 
time <- toc()
fit_time_models <- append(fit_time_models,time[[4]])

'Logistic Tree    Random Forest XGB    KNN'
'3.99   4.521  7.458       2.418  16.544'

```

## Compare models

```{r, compare_models}
## Model Comparison

log_metrics <- 
  log_res %>% 
  collect_metrics(summarise = TRUE) %>%
  # add the name of the model to every row
  mutate(model = "Logistic Regression",
         time = log_time)

tree_metrics <- 
  tree_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Decision Tree")

rf_metrics <- 
  rf_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Random Forest")

xgb_metrics <- 
  xgb_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "XGBoost")

knn_metrics <- 
  knn_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Knn")

# add more models here

# create dataframe with all models
model_compare <- bind_rows(log_metrics,
                            tree_metrics,
                            rf_metrics,
                           xgb_metrics,
                           knn_metrics
                      ) %>% 
  # get rid of 'sec elapsed' and turn it into a number
  mutate(time = str_sub(time, end = -13) %>% 
           as.double()
         )


#Pivot wider to create barplot
  model_comp <- model_compare %>% 
  select(model, .metric, mean, std_err) %>% 
  pivot_wider(names_from = .metric, values_from = c(mean, std_err)) 

# show mean are under the curve (ROC-AUC) for every model
model_comp %>% 
  arrange(mean_roc_auc) %>% 
  mutate(model = fct_reorder(model, mean_roc_auc)) %>% # order results
  ggplot(aes(model, mean_roc_auc, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") +
   geom_text(
     size = 3,
     aes(label = round(mean_roc_auc, 2), 
         y = mean_roc_auc + 0.08),
     vjust = 1
  )+
  theme_light()+
  theme(legend.position = "none")+
  labs(y = NULL)


log_pred <- log_res %>% collect_predictions()
log_pred %>% 
  roc_curve(is_fraud, .pred_1) %>% 
  autoplot() +
  labs(title="Logisitic Regression ROC")

tree_pred <- tree_res %>% collect_predictions()
tree_pred %>% 
  roc_curve(is_fraud, .pred_1) %>% 
  autoplot() +
  labs(title="Decision Tree ROC")

rf_pred <- rf_res %>% collect_predictions()
rf_pred %>% 
  roc_curve(is_fraud, .pred_1) %>% 
  autoplot() +
  labs(title="Random Forest ROC")


xgb_pred <- xgb_res %>% collect_predictions()
xgb_pred %>% 
  roc_curve(is_fraud, .pred_1) %>% 
  autoplot() +
  labs(title="XGB ROC")


knn_pred <- knn_res %>% collect_predictions()
knn_pred %>% 
  roc_curve(is_fraud, .pred_1) %>% 
  autoplot() +
  labs(title="KNN ROC")
```

## Which metric to use

This is a highly imbalanced data set, as roughly 99.5% of all transactions are ok, and it's only 0.5% of transactions that are fraudulent. A `naive` model, which classifies everything as ok and not-fraud, would have an accuracy of 99.5%, but what about the sensitivity, specificity, the AUC, etc?
```{=html}
<span style="color:red">
<ul>
  <li>
    <p>Since accuracy is very high for all models, specficity is the more important metric to consider. This is the true negative rate. It is calculated as the ratio of true negatives (TN) to the sum of true negatives and false positives (FP). A high specificity indicates that the model is good at identifying negative cases correctly </p>
        <p>Area Under Curve is a metric that combines sensitivity and specificity.The ROC curve is a graphical representation of the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) for different classification thresholds. AUC-ROC measures the overall performance of the classifier across all possible thresholds. A higher AUC-ROC indicates better model performance.  </p>
  </li>
    <p>In our training data we see that XGBoost and Random Forest have the same AUC score. We have decided to choose the XGB model to apply to the test set because XGBoost uses the gradient descent algorithm meaning that it improves on itself to solve for the local minima.</p>
  </li>
</ul>
</span>
```



## `last_fit()`
```{r}

last_fit_xgb <- last_fit(xgb_wflow, 
                        split = data_split,
                        metrics = metric_set(
                          accuracy, f_meas, kap, precision,
                          recall, roc_auc, sens, spec))

last_fit_xgb %>% collect_metrics(summarize = TRUE)

#Compare to training
xgb_res %>% collect_metrics(summarize = TRUE)
```

## Get variable importance using `vip` package


```{r}

library(vip)

last_fit_xgb %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 100) +
  theme_light()

## Amount and category_grocery are the most important features, 
## followed by age and whether the transaction was made between 10-11pm

```

## Plot Final Confusion matrix and ROC curve


```{r}
## Final Confusion Matrix

last_fit_xgb %>%
  collect_predictions() %>% 
  conf_mat(is_fraud, .pred_class) %>% 
  autoplot(type = "heatmap")


## Final ROC curve
last_fit_xgb %>% 
  collect_predictions() %>% 
  roc_curve(is_fraud, .pred_1) %>% 
  autoplot() +
  labs(title="XGB ROC Test")

```


```{r}
#Assessing which model affords the best classification from these results:
#
# First, considering the results from each model in turn: 
#
# The logistic regression (LR) performs reasonably, as judged by the fact that the ROC is decent (0.83) although the model has low sensitivity while having high specificity. Meanwhile, the precision and recall (and thus F-score) metrics are very low (in the 30% territory); so there is little discrimination between any elements in the confusion matrix; no particular fold is far better than any other. So, this is not a good model. 
#
# The decision tree model fares slightly worse in the ROC metric (0.78) but it has better sensitivity than the LR model. The evaluation metrics are also better, e.g. precision (0.65) and recall (0.47). This suggests that the model does fit some of the data with a priori information better.
#
# The random forest results compare more favourably, being the best of all models tested in terms of the ROC result (0.94). The model has perfect precision but almost zero recall and thus a poor F-score. It also has poor sensitivity.
#
# The bootstrapping decision-tree option, gradient boosting, fares similarly to that of the random forest model (ROC  = 0.93), but overall evaluation metrics look a lot better and are more balanced. For example, the precision and recall are 69 and 46% respectively. The sensitivity (46%) and specificity (100%) are also much better and more balanced. So the Gradient boosting method results look particularly good.
#
# the k-nearest neighbour classification model does not perform so well (ROC = 0.71); it is derived from good specificity (100% = perfect) but low sensitivity (21%). Meanwhile, the statistical figure-of-merits, precision (30%) and recall (21%) are very low. So, this is not a good model.
#
# In summary, the gradient boosting model is clearly the best model. Others suffer in various regards, in particularly some are very imbalanced between precision versus recall, and/or sensitivity versus specificity. It makes sense that the gradient boosting (bootstrapping) model offers the best fit to the training data. This is because the random forest model will train many trees independently while a gradient boosting model will train many trees subsequently (correcting the errors in a given tree from that of its forerunner tree). Thus, the gradient boosting model has bootstrapping functionality, and its results are easier to interpret than that of a random forest model because gradient boosting models are afforded from one final tree while the result of a random forest model are the ensemble of many trees that have been modelled in parallel. Given these fundamental model considerations, and the similar ROC of these two model options, the gradient boosting model was deemed to be the best model for being taken forward in classifying the test (unseen) movie data. 
#
#The results of applying these test data to the gradient boosting model are good, albeit less good than the results from the training data. This comparison stands to reason since unseen data will naturally fit less well than the training data which were used to create (i.e. were tailored to the fit of) the model. Besides, the results of testing and training data still fare well in having fairly small overall differences.
#

```


##  Calculating the cost of fraud to the company


- How much money (in US\$ terms) are fraudulent transactions costing the company? Generate a table that summarizes the total amount of legitimate and fraudulent transactions per year and calculate the % of fraudulent transactions, in US\$ terms. Compare your model vs the naive classification that we do not have any fraudulent transactions. 

```{r}
#| label: savings-for-cc-company

best_model_wflow <- xgb_wflow

best_model_preds <- 
  best_model_wflow %>% 
  fit(data = card_fraud_train) %>%  
  
  ## Use `augment()` to get predictions for entire data set
  augment(new_data = card_fraud)

best_model_preds %>% 
  conf_mat(truth = is_fraud, estimate = .pred_class)

cost <- best_model_preds %>%
  select(is_fraud, amt, pred = .pred_class) 

cost <- cost %>%
  mutate(
    false_naives = if_else((is_fraud ==1),amt,0), 
    false_negatives=if_else((is_fraud ==1)&(pred ==0),amt,0),
    false_positives=if_else((is_fraud ==0)&(pred ==1),amt,0),
    true_positives=0,
    true_negatives=0


  

  # naive false-- we think every single transaction is ok and not fraud


  # false negatives-- we thought they were not fraud, but they were

  
  
  # false positives-- we thought they were fraud, but they were not

  
    
  # true positives-- we thought they were fraud, and they were 


  
  # true negatives-- we thought they were ok, and they were 
)
  
# Summarising

cost_summary <- cost %>% 
  summarise(across(starts_with(c("false","true", "amt")), 
            ~ sum(.x, na.rm = TRUE)))

cost_summary
  
print(scales::dollar(cost_summary$false_naives - cost_summary$false_negatives - cost_summary$false_positives * 0.02))
#Implementing our model will save $1,023,627

```


- If we use a naive classifier thinking that all transactions are legitimate and not fraudulent, the cost to the company is `r scales::dollar(cost_summary$false_naives)`.
- With our best model, the total cost of false negatives, namely transactions our classifier thinks are legitimate but which turned out to be fraud, is `r scales::dollar(cost_summary$false_negatives)`.

- Our classifier also has some false positives, `r scales::dollar(cost_summary$false_positives)`, namely flagging transactions as fraudulent, but which were legitimate. Assuming the card company makes around 2% for each transaction (source: https://startups.co.uk/payment-processing/credit-card-processing-fees/), the amount of money lost due to these false positives is `r scales::dollar(cost_summary$false_positives * 0.02)`

- The \$ improvement over the naive policy is `r scales::dollar(cost_summary$false_naives - cost_summary$false_negatives - cost_summary$false_positives * 0.02)`.
